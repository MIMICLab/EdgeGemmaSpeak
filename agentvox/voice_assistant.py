import os
import torch
import numpy as np
import re
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from pathlib import Path
import time
import threading
import logging
import base64
import io
import math
import ctypes
from PIL import Image


import ctypes
from typing import (
    List,
    Literal,
    Tuple,
)

# PyTorch 2.6 security settings
import warnings
warnings.filterwarnings("ignore", message="torch.load warnings")

# Ignore numpy RuntimeWarning (divide by zero, overflow, invalid value)
np.seterr(divide='ignore', invalid='ignore', over='ignore')

# Libraries for speech recognition and synthesis
from RealtimeSTT import AudioToTextRecorder
from RealtimeTTS import TextToAudioStream, CoquiEngine

# Libraries for LLM
from llama_cpp import Llama
import llama_cpp.llama as llama
import llama_cpp
from llama_cpp.llama_chat_format import Llava15ChatHandler
from pathlib import Path
from contextlib import redirect_stderr

# Libraries for audio
import pygame
import soundfile as sf
import tempfile
import subprocess
import platform

# Gemma3 Chat Handler for multimodal support
class Gemma3ChatHandler(Llava15ChatHandler):
    # Chat Format:
    # '<bos><start_of_turn>user\n{system_prompt}\n\n{prompt}<end_of_turn>\n<start_of_turn>model\n'

    DEFAULT_SYSTEM_MESSAGE = None

    CHAT_FORMAT = (
        "{{ '<bos>' }}"
        "{%- if messages[0]['role'] == 'system' -%}"
        "{%- if messages[0]['content'] is string -%}"
        "{%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}"
        "{%- else -%}"
        "{%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}"
        "{%- endif -%}"
        "{%- set loop_messages = messages[1:] -%}"
        "{%- else -%}"
        "{%- set first_user_prefix = \"\" -%}"
        "{%- set loop_messages = messages -%}"
        "{%- endif -%}"
        "{%- for message in loop_messages -%}"
        "{%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}"
        "{{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}"
        "{%- endif -%}"
        "{%- if (message['role'] == 'assistant') -%}"
        "{%- set role = \"model\" -%}"
        "{%- else -%}"
        "{%- set role = message['role'] -%}"
        "{%- endif -%}"
        "{{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}"
        "{%- if message['content'] is string -%}"
        "{{ message['content'] | trim }}"
        "{%- elif message['content'] is iterable -%}"
        "{%- for item in message['content'] -%}"
        "{%- if item['type'] == 'image_url' -%}"
        "{{ '<start_of_image>' }}"
        "{%- elif item['type'] == 'text' -%}"
        "{{ item['text'] | trim }}"
        "{%- endif -%}"
        "{%- endfor -%}"
        "{%- else -%}"
        "{{ raise_exception(\"Invalid content type\") }}"
        "{%- endif -%}"
        "{{ '<end_of_turn>\n' }}"
        "{%- endfor -%}"
        "{%- if add_generation_prompt -%}"
        "{{ '<start_of_turn>model\n' }}"
        "{%- endif -%}"
    )

    @staticmethod
    def split_text_on_image_urls(text: str, image_urls: List[str]):
        split_text: List[Tuple[Literal["text", "image_url"], str]] = []
        copied_urls = image_urls[:]
        remaining = text
        image_placeholder = "<start_of_image>"

        while remaining:
            # Find placeholder
            pos = remaining.find(image_placeholder)
            if pos != -1:
                assert len(copied_urls) > 0
                if pos > 0:
                    split_text.append(("text", remaining[:pos]))
                split_text.append(("text", "\n\n<start_of_image>"))
                split_text.append(("image_url", copied_urls.pop(0)))
                split_text.append(("text", "<end_of_image>\n\n"))
                remaining = remaining[pos + len(image_placeholder):]
            else:
                assert len(copied_urls) == 0
                split_text.append(("text", remaining))
                remaining = ""
        return split_text

    def eval_image(self, llama: llama.Llama, image_url: str):

        n_tokens = 256
        if llama.n_tokens + n_tokens > llama.n_ctx():
            raise ValueError(
                f"Prompt exceeds n_ctx: {llama.n_tokens + n_tokens} > {llama.n_ctx()}"
            )

        img_bytes = self.load_image(image_url)
        img_u8_p = self._llava_cpp.clip_image_u8_init()
        if not self._llava_cpp.clip_image_load_from_bytes(
            ctypes.create_string_buffer(img_bytes, len(img_bytes)),
            ctypes.c_size_t(len(img_bytes)),
            img_u8_p,
        ):
            self._llava_cpp.clip_image_u8_free(img_u8_p)
            raise ValueError("Failed to load image.")

        img_f32 = self._llava_cpp.clip_image_f32_batch()
        img_f32_p = ctypes.byref(img_f32)
        if not self._llava_cpp.clip_image_preprocess(self.clip_ctx, img_u8_p, img_f32_p):
            self._llava_cpp.clip_image_f32_batch_free(img_f32_p)
            self._llava_cpp.clip_image_u8_free(img_u8_p)
            raise ValueError("Failed to preprocess image.")

        n_embd = llama_cpp.llama_model_n_embd(llama._model.model)
        embed = (ctypes.c_float * (n_tokens * n_embd))()
        if not self._llava_cpp.clip_image_batch_encode(self.clip_ctx, llama.n_threads, img_f32_p, embed):
            self._llava_cpp.clip_image_f32_batch_free(img_f32_p)
            self._llava_cpp.clip_image_u8_free(img_u8_p)
            raise ValueError("Failed to encode image.")

        self._llava_cpp.clip_image_f32_batch_free(img_f32_p)
        self._llava_cpp.clip_image_u8_free(img_u8_p)
        llama_cpp.llama_set_causal_attn(llama.ctx, False)

        seq_id_0 = (ctypes.c_int32 * 1)()
        seq_ids = (ctypes.POINTER(ctypes.c_int32) * (n_tokens + 1))()
        for i in range(n_tokens):
            seq_ids[i] = seq_id_0

        batch = llama_cpp.llama_batch()
        batch.n_tokens = n_tokens
        batch.token = None
        batch.embd = embed
        batch.pos = (ctypes.c_int32 * n_tokens)(*[i + llama.n_tokens for i in range(n_tokens)])
        batch.seq_id = seq_ids
        batch.n_seq_id = (ctypes.c_int32 * n_tokens)(*([1] * n_tokens))
        batch.logits = (ctypes.c_int8 * n_tokens)()

        if llama_cpp.llama_decode(llama.ctx, batch):
            raise ValueError("Failed to decode image.")

        llama_cpp.llama_set_causal_attn(llama.ctx, True)
        # Required to avoid issues with hf tokenizer
        llama.input_ids[llama.n_tokens : llama.n_tokens + n_tokens] = -1
        llama.n_tokens += n_tokens

def image_to_base64_data_uri(image: Image.Image, format: str = "JPEG", quality: int = 85) -> str:
    """Convert PIL Image to base64 data URI."""
    buffered = io.BytesIO()
    image.save(buffered, format=format, quality=quality, optimize=True)
    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
    mime_type = f"image/{format.lower()}"
    return f'data:{mime_type};base64,{img_base64}'

@dataclass
class AudioConfig:
    """Class for managing audio configuration"""
    sample_rate: int = 16000
    channels: int = 1
    chunk_size: int = 2048
    audio_format: str = "wav"
    
@dataclass
class ModelConfig:
    """Class for managing model configuration"""
    stt_model: str = "base"  # Whisper model size
    llm_model: str = None  # Local GGUF model path (uses default model if None)
    mmproj_model: str = None  # Multimodal projection model path (for vision)
    tts_model: str = "tts_models/multilingual/multi-dataset/xtts_v2"  # XTTS v2 multilingual model
    device: str = "auto"  # Device: auto, cpu, cuda, mps
    is_multimodal: bool = False  # Enable multimodal (vision) support
    
    # STT detailed settings
    stt_language: str = "ko"
    stt_beam_size: int = 5
    stt_temperature: float = 0.0
    stt_vad_threshold: float = 0.5
    stt_vad_min_speech_duration_ms: int = 250
    stt_vad_min_silence_duration_ms: int = 1000  # Reduced from 2000ms for faster response
    
    # TTS detailed settings
    tts_engine: str = "coqui"  # Using Coqui engine
    speaker_wav: Optional[str] = None  # Voice cloning source file
    tts_speed: float = 1.0  # TTS speed (1.0 is normal, higher is faster)
    
    # LLM detailed settings
    llm_max_tokens: int = 512
    llm_temperature: float = 0.7
    llm_top_p: float = 0.95
    llm_repeat_penalty: float = 1.1
    llm_context_size: int = 4096
    
    def __post_init__(self):
        """Auto-detect device after initialization"""
        if self.device == "auto":
            import torch
            if torch.cuda.is_available():
                self.device = "cuda"
                print(f"Auto-detected device: CUDA (GPU: {torch.cuda.get_device_name(0)})")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"
                print("Auto-detected device: Apple Silicon (MPS)")
            else:
                self.device = "cpu"
                print("Auto-detected device: CPU")

class STTModule:
    """Module for converting speech to text using RealtimeSTT"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        
        # Initialize RealtimeSTT recorder - simplified
        self.recorder = AudioToTextRecorder(
            model=config.stt_model,
            language=config.stt_language,
            device=config.device,
            spinner=False,
            use_microphone=True,
            level=logging.WARNING
        )
        
    def transcribe_once(self) -> Optional[str]:
        """Listen and transcribe once"""
        is_korean = self.config.stt_language.startswith('ko')
        
        if is_korean:
            print("\në§ì”€í•´ì£¼ì„¸ìš”...")
        else:
            print("\nPlease speak...")
            
        # Simply get text
        text = self.recorder.text()
        
        if text:
            print(f"\nì‚¬ìš©ì: {text}" if is_korean else f"\nUser: {text}")
            return text
        return None

class LlamaTokenizer:
    def __init__(self, llama_model):
        self._llama = llama_model

    def __call__(self, text, add_bos=True, return_tensors=None):
        ids = self._llama.tokenize(text, add_bos=add_bos)
        if return_tensors == "pt":
            return torch.tensor([ids])
        return ids

    def decode(self, ids):
        return self._llama.detokenize(ids).decode("utf-8", errors="ignore")

class LLMModule:
    """Local LLM response generation module using Llama.cpp with multimodal support"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.device = config.device
        self.is_multimodal = config.is_multimodal
        
        # Set default model path if not provided
        if config.llm_model is None:
            # Look for model in package data or user directory
            package_dir = Path(__file__).parent.absolute()
            model_filename = "gemma-3-12b-it-Q4_K_M.gguf"
            
            # Check in package directory first
            package_model_path = package_dir / "models" / model_filename
            if package_model_path.exists():
                self.model_path = str(package_model_path)
            else:
                # Check in user home directory
                home_model_path = Path.home() / ".agentvox" / "models" / model_filename
                if home_model_path.exists():
                    self.model_path = str(home_model_path)
                else:
                    raise FileNotFoundError(
                        f"Model file not found. Please download {model_filename} and place it in:\n"
                        f"1. {package_model_path} or\n"
                        f"2. {home_model_path}\n"
                        f"Or provide the model path explicitly."
                    )
        else:
            # Convert relative path to absolute path
            if not os.path.isabs(config.llm_model):
                current_dir = Path(__file__).parent.absolute()
                self.model_path = str(current_dir / config.llm_model)
            else:
                self.model_path = config.llm_model
        
        # Set up multimodal projection model path
        self.mmproj_path = None
        chat_handler = None
        
        if self.is_multimodal and config.mmproj_model:
            if not os.path.isabs(config.mmproj_model):
                # If relative path, resolve it relative to model directory
                model_dir = Path(self.model_path).parent
                self.mmproj_path = str(model_dir / config.mmproj_model)
            else:
                self.mmproj_path = config.mmproj_model
            
            # Check if mmproj file exists
            if not os.path.exists(self.mmproj_path):
                # Try default mmproj filename
                model_dir = Path(self.model_path).parent
                default_mmproj = model_dir / "mmproj-gemma-3-12b-it-F16.gguf"
                if default_mmproj.exists():
                    self.mmproj_path = str(default_mmproj)
                else:
                    raise FileNotFoundError(f"Multimodal projection model not found: {self.mmproj_path}")
            
            # Initialize chat handler for multimodal
            chat_handler = Gemma3ChatHandler(clip_model_path=self.mmproj_path, verbose=False)
        
        # Load Llama model
        with open(os.devnull, 'w') as devnull:
            with redirect_stderr(devnull):
                self.model = Llama(
                    model_path=self.model_path,
                    n_gpu_layers=-1,  # Load all layers to GPU
                    n_ctx=self.config.llm_context_size,      # Context size
                    verbose=False,
                    flash_attn=True,   # Use Flash Attention
                    chat_handler=chat_handler  # Add chat handler for multimodal
                )
                self.tokenizer = LlamaTokenizer(self.model)
        
        # Manage conversation history
        self.conversation_history = []
        
    def generate_response(self, text: str, images: Optional[List[Image.Image]] = None, max_length: int = 512) -> str:
        """Generate response for input text, optionally with images for multimodal models"""
        # Check if using Korean voice
        is_korean = self.config.stt_language.startswith('ko')
        
        # Build conversation context
        if is_korean:
            self.conversation_history.append(f"ì‚¬ìš©ì: {text}")
        else:
            self.conversation_history.append(f"User: {text}")
        
        # Handle multimodal input
        if images is not None and self.is_multimodal:
            try:
                # Merge images into a grid
                image = images[-1]
                # Convert image to data URI
                image_uri = image_to_base64_data_uri(image, format="JPEG", quality=90)
                
                # Use chat completion API for multimodal input
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {'type': 'text', 'text': text},
                            {'type': 'image_url', 'image_url': {'url': image_uri}}
                        ]
                    }
                ]
                
                response = self.model.create_chat_completion(
                    messages=messages,
                    stop=['<end_of_turn>', '<eos>'],
                    max_tokens=max_length if max_length != 512 else self.config.llm_max_tokens,  # Reduce for multimodal
                    temperature=self.config.llm_temperature,
                    top_p=self.config.llm_top_p,
                    repeat_penalty=self.config.llm_repeat_penalty
                )
                response_text = response['choices'][0]['message']['content'].strip()
            except Exception as e:
                # Fallback to text-only if multimodal fails
                is_korean = self.config.stt_language.startswith('ko')
                if is_korean:
                    print(f"âš ï¸ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤: {e}")
                else:
                    print(f"âš ï¸ Multimodal processing failed, falling back to text-only: {e}")
                
                # Process as text-only
                prompt = self._build_prompt()
                answer = self.model(
                    prompt,
                    stop=['<end_of_turn>', '<eos>'],
                    max_tokens=max_length if max_length != 512 else self.config.llm_max_tokens,
                    echo=False,
                    temperature=self.config.llm_temperature,
                    top_p=self.config.llm_top_p,
                    repeat_penalty=self.config.llm_repeat_penalty,
                )
                response_text = answer['choices'][0]['text'].strip()
        else:
            # Text-only generation
            prompt = self._build_prompt()
            
            # Generate response
            answer = self.model(
                prompt,
                stop=['<end_of_turn>', '<eos>'],
                max_tokens=max_length if max_length != 512 else self.config.llm_max_tokens,
                echo=False,
                temperature=self.config.llm_temperature,
                top_p=self.config.llm_top_p,
                repeat_penalty=self.config.llm_repeat_penalty,
            )
            
            response_text = answer['choices'][0]['text'].strip()
        
        response = response_text
        
        # Check if using Korean voice
        is_korean = self.config.stt_language.startswith('ko')
        
        # Remove "Assistant:" or "ì–´ì‹œìŠ¤í„´íŠ¸:" prefix
        if response.startswith("Assistant:"):
            response = response[10:].strip()
        elif response.startswith("ì–´ì‹œìŠ¤í„´íŠ¸:"):
            response = response[6:].strip()
        
        # Handle empty response or response with only special characters
        if not response or not re.search(r'[\uac00-\ud7a3a-zA-Z0-9]', response):
            if is_korean:
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
            else:
                response = "I'm sorry. Could you please say that again?"
        
        # Add to conversation history
        if is_korean:
            self.conversation_history.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {response}")
        else:
            self.conversation_history.append(f"Assistant: {response}")
        
        # Remove old history if too long (keep 20 turns)
        if len(self.conversation_history) > 20:
            self.conversation_history = self.conversation_history[-20:]
            
        return response
    
    def _build_prompt(self) -> str:
        """Build prompt with conversation context"""
        # Check if using Korean voice
        is_korean = self.config.stt_language.startswith('ko')
        
        # System prompt
        if is_korean:
            system_prompt = """ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµ ë¯¸ë¯¹ë©ì—ì„œ ê°œë°œí•œ ì‹œê° ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì´ë¯¸ì§€ ë¶„ì„ ê·œì¹™:
- ì œê³µëœ ì´ë¯¸ì§€ëŠ” ì‚¬ìš©ìì˜ í˜„ì¬ ì‹œì•¼ì…ë‹ˆë‹¤
- ì´ˆë¡ìƒ‰ ì ì€ ì‚¬ìš©ìê°€ ë³´ê³  ìˆëŠ” ìœ„ì¹˜ë¥¼ í‘œì‹œí•œ ê²ƒì…ë‹ˆë‹¤ (ì‹œìŠ¤í…œì´ ì¶”ê°€í•œ ë§ˆì»¤)
- ì´ˆë¡ìƒ‰ ì  ìì²´ë¥¼ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ ìœ„ì¹˜ì— ìˆëŠ” ì‹¤ì œ ê°ì²´ë‚˜ ë‚´ìš©ì„ ì„¤ëª…í•˜ì„¸ìš”
- ë©”íƒ€ ì§ˆë¬¸ì´ë‚˜ ì„¤ëª… ì—†ì´ ë°”ë¡œ ë‹µë³€í•˜ì„¸ìš”

ì‘ë‹µ ê·œì¹™:
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€
- í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€
- "ì‚¬ì§„", "ì´ë¯¸ì§€", "í™”ë©´" ê°™ì€ ë‹¨ì–´ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šê¸°
- ë³„í‘œ(*), í•˜ì´í”ˆ(-), ì½œë¡ (:) ë“± íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš©í•˜ì§€ ì•Šê¸°
- ë¦¬ìŠ¤íŠ¸ë‚˜ ê°•ì¡° í‘œì‹œ ì—†ì´ ì¼ë°˜ ë¬¸ì¥ìœ¼ë¡œë§Œ ë‹µë³€
- ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ ì§ˆë¬¸ í•˜ì§€ ì•Šê¸°
- ì˜ì–´ëŠ” í•œê¸€ë¡œ í‘œê¸° (ì˜ˆ: AIâ†’ì—ì´ì•„ì´)"""
        else:
            system_prompt = """You are a visual assistant developed by MimicLab at Sogang University.

Image Analysis Rules:
- The provided image is the user's current view
- The green dot is a system marker showing where the user is looking
- Don't mention the green dot itself, describe the actual object or content at that location
- Answer directly without meta questions or explanations

Response Rules:
- Be concise and clear
- Never use words like "photo", "image", "picture", "screen"
- No special characters like asterisks (*), hyphens (-), colons (:)
- No lists or formatting, only plain sentences
- Plain text only, no markdown or emoticons"""
        
        # Build prompt with full conversation history
        conversation_text = ""
        
        # If first conversation
        if len(self.conversation_history) == 1:
            conversation_text = f"<start_of_turn>user\n{system_prompt}\n\n{self.conversation_history[0]}\n<end_of_turn>\n<start_of_turn>model\n"
        else:
            # Include system prompt
            conversation_text = f"<start_of_turn>user\n{system_prompt}\n<end_of_turn>\n"
            
            # Include previous conversation history
            for turn in self.conversation_history:
                if turn.startswith("User:") or turn.startswith("ì‚¬ìš©ì:"):
                    conversation_text += f"<start_of_turn>user\n{turn}\n<end_of_turn>\n"
                elif turn.startswith("Assistant:") or turn.startswith("ì–´ì‹œìŠ¤í„´íŠ¸:"):
                    conversation_text += f"<start_of_turn>model\n{turn}\n<end_of_turn>\n"
            
            # End with model turn
            conversation_text += "<start_of_turn>model\n"
        
        return conversation_text

    def reset_conversation(self):
        """Reset conversation history"""
        self.conversation_history = []

class TTSModule:
    """TTS module using RealtimeTTS with CoquiEngine"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        
        # Initialize Coqui engine
        self.engine = CoquiEngine(
            model_name=config.tts_model,
            device=config.device,
            voice=config.speaker_wav,
            language=config.stt_language,
            speed=config.tts_speed
        )
        
        # Initialize text-to-audio stream
        self.stream = TextToAudioStream(self.engine)
        
    def speak(self, text: str):
        """Speak text and wait until complete"""
        if not text or not text.strip():
            return
            
        try:
            # Feed and play - blocking call
            self.stream.feed(text)
            self.stream.play()
                
        except Exception as e:
            print(f"TTS error: {e}")
            

class VoiceAssistant:
    """Main class for managing the entire voice conversation system"""
    
    def __init__(self, model_config: ModelConfig, audio_config: AudioConfig):
        self.model_config = model_config
        self.audio_config = audio_config
        
        # External audio source (for Aria integration)
        self.external_audio_source = None
        self.use_external_audio = False
        
        is_korean = model_config.stt_language.startswith('ko')
        
        if is_korean:
            print("ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
            if model_config.is_multimodal:
                print("ë©€í‹°ëª¨ë‹¬(ë¹„ì „) ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("Initializing models...")
            if model_config.is_multimodal:
                print("Multimodal (vision) capabilities enabled.")
            
        self.stt = STTModule(model_config)
        self.llm = LLMModule(model_config)
        self.tts = TTSModule(model_config)
        
        # Image buffer for multimodal input
        self.image_buffer = []
    
    def add_image(self, image: Image.Image):
        """Add an image to the buffer for multimodal input"""
        if not self.model_config.is_multimodal:
            print("Warning: Multimodal support is not enabled. Image will be ignored.")
            return
        
        self.image_buffer.append(image)
        is_korean = self.model_config.stt_language.startswith('ko')
        if is_korean:
            print(f"ì´ë¯¸ì§€ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ {len(self.image_buffer)}ê°œì˜ ì´ë¯¸ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"Image added. Currently {len(self.image_buffer)} images in buffer.")
    
    def add_image_from_path(self, image_path: str):
        """Add an image from file path to the buffer"""
        try:
            image = Image.open(image_path)
            self.add_image(image)
        except Exception as e:
            is_korean = self.model_config.stt_language.startswith('ko')
            if is_korean:
                print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                print(f"Failed to load image: {e}")
    
    def clear_images(self):
        """Clear all images from the buffer"""
        self.image_buffer = []
        is_korean = self.model_config.stt_language.startswith('ko')
        if is_korean:
            print("ì´ë¯¸ì§€ ë²„í¼ê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("Image buffer cleared.")
    
    def set_external_audio_source(self, audio_source):
        """Set external audio source (e.g., Aria glasses)"""
        self.external_audio_source = audio_source
        self.use_external_audio = True
        is_korean = self.model_config.stt_language.startswith('ko')
        if is_korean:
            print("ì™¸ë¶€ ì˜¤ë””ì˜¤ ì†ŒìŠ¤(Aria)ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("External audio source (Aria) configured.")
    
    def listen_from_external_audio(self) -> Optional[str]:
        """Listen to audio from external source (Aria) and transcribe"""
        import time
        
        is_korean = self.model_config.stt_language.startswith('ko')
        
        if is_korean:
            print("\nğŸ¤ Aria ë§ˆì´í¬ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”...")
        else:
            print("\nğŸ¤ Please speak into Aria microphone...")
        
        # Collect audio chunks from Aria
        audio_chunks = []
        silence_count = 0
        max_silence = 50  # About 5 seconds of silence
        
        while True:
            # Get audio chunk from Aria
            chunk = self.external_audio_source.get_audio_chunk(1600)  # 1600 samples â‰ˆ 33ms at 48kHz
            
            if chunk is not None:
                audio_chunks.append(chunk)
                # Reset silence counter if we got audio
                silence_count = 0
            else:
                silence_count += 1
                if silence_count > max_silence:
                    break
                time.sleep(0.1)  # Wait 100ms before next check
        
        if audio_chunks:
            # Concatenate all chunks
            import numpy as np
            audio_data = np.concatenate(audio_chunks)
            
            # Feed to RealtimeSTT
            self.stt.recorder.feed_audio(audio_data)
            
            # Get transcription
            return self.stt.recorder.text()
        
        return None
    
    def run_conversation_loop(self):
        """Run conversation loop - simple version"""
        is_korean = self.model_config.stt_language.startswith('ko')
        
        if is_korean:
            print("ìŒì„± ëŒ€í™” ì‹œìŠ¤í…œì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¼ê³  ë§í•˜ì„¸ìš”.")
        else:
            print("Voice conversation system started.")
            print("Say 'exit' to quit.")
        print("-" * 50)
        
        while True:
            # 1. Listen to user
            if self.use_external_audio and self.external_audio_source:
                # Use external audio source (Aria)
                user_input = self.listen_from_external_audio()
            else:
                # Use computer microphone
                user_input = self.stt.transcribe_once()
            
            if not user_input:
                continue
                
            # Check exit command
            if "exit" in user_input.lower() or "ì¢…ë£Œ" in user_input:
                if is_korean:
                    print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                else:
                    print("\nEnding conversation.")
                break
            
            # 2. Get LLM response (with images if available)
            images_to_use = self.image_buffer if self.image_buffer else None
            if images_to_use:
                if is_korean:
                    print(f"\n{len(images_to_use)}ê°œì˜ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘...")
                else:
                    print(f"\nGenerating response with {len(images_to_use)} images...")
            
            response = self.llm.generate_response(user_input, images=images_to_use)
            print(f"\nì–´ì‹œìŠ¤í„´íŠ¸: {response}" if is_korean else f"\nAssistant: {response}")
            
            # Clear images after use
            if images_to_use:
                self.clear_images()
            
            # 3. Speak response - this blocks until complete
            self.tts.speak(response)
            
            # 4. Loop back to listening
            # No need for delays or complex state management

# Main execution function
def main():
    """Main execution function"""
    # Initialize configuration
    audio_config = AudioConfig()
    model_config = ModelConfig()
    
    # Initialize voice assistant
    assistant = VoiceAssistant(model_config, audio_config)
    
    # Run console conversation mode
    assistant.run_conversation_loop()

if __name__ == "__main__":
    # Required for multiprocessing on macOS/Windows
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    main()